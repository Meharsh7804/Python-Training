{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Prerequisites"
      ],
      "metadata": {
        "id": "F8eafgtYpI6P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5BGIcCpmIHX",
        "outputId": "517b8fbd-d2ba-4730-9aec-aa08b37c6839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.173.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.6.15)\n"
          ]
        }
      ],
      "source": [
        "pip install google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Setup Gemini API"
      ],
      "metadata": {
        "id": "SWAlSBCJpMAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "genai.configure(api_key=\"AIzaSyB1u9b3ssnrmGml_gHy6nBw87mfX-jyhgI\")"
      ],
      "metadata": {
        "id": "jA_ZtciHmsMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Initialize Gemini Model"
      ],
      "metadata": {
        "id": "M2Za2_DWpRWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-1.5-flash')"
      ],
      "metadata": {
        "id": "w2ac8eyNoL7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Zero-shot Prompting"
      ],
      "metadata": {
        "id": "-6v3U9LNpTrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero-shot: Ask a question without any examples\n",
        "prompt_zero = \"Translate this sentence into hindi: 'Good morning, how are you?'\"\n",
        "\n",
        "response_zero = model.generate_content(prompt_zero)\n",
        "print(\"üü° Zero-shot Output:\\n\", response_zero.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "zcQo-R8OodoL",
        "outputId": "f57e5c56-9f13-413e-8a0d-23b9864f379e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üü° Zero-shot Output:\n",
            " The most common and natural translation of \"Good morning, how are you?\" in Hindi is:\n",
            "\n",
            "**‡§∏‡•Å‡§™‡•ç‡§∞‡§≠‡§æ‡§§, ‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç? (Suprabhaat, aap kaise hain?)**\n",
            "\n",
            "Here's a breakdown:\n",
            "\n",
            "* **‡§∏‡•Å‡§™‡•ç‡§∞‡§≠‡§æ‡§§ (Suprabhaat):** Good morning\n",
            "* **‡§Ü‡§™ (aap):** You (formal, used for showing respect)\n",
            "* **‡§ï‡•à‡§∏‡•á (kaise):** How\n",
            "* **‡§π‡•à‡§Ç (hain):** are (plural formal verb)\n",
            "\n",
            "\n",
            "While you could use \"tum kaise ho?\" (‡§§‡•Å‡§Æ ‡§ï‡•à‡§∏‡•á ‡§π‡•ã?) which uses the informal \"tum\" (you),  it's generally better to use the formal \"aap\" unless you're speaking to a close friend or family member.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Few-shot Prompting"
      ],
      "metadata": {
        "id": "dd9LcR0CpWKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üü¢ Few-shot: Give a few examples in the prompt\n",
        "prompt_few = \"\"\"\n",
        "Translate these sentences to hinid:\n",
        "1. Hello, how are you? ‚Üí Bonjour, comment √ßa va ?\n",
        "2. What is your name? ‚Üí Comment tu t'appelles ?\n",
        "3. I love programming. ‚Üí\n",
        "\"\"\"\n",
        "\n",
        "response_few = model.generate_content(prompt_few)\n",
        "print(\"\\nüü¢ Few-shot Output:\\n\", response_few.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "vaWuf7Y2onO8",
        "outputId": "221cf7ab-f6a7-4248-ccde-80ce4decf721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üü¢ Few-shot Output:\n",
            " The first two sentences are in French, not English.  Here's the translation of all three, assuming the third is in English:\n",
            "\n",
            "\n",
            "1. **Hello, how are you? (Bonjour, comment √ßa va ?)**  ‚Üí ‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§ï‡•à‡§∏‡•á ‡§π‡•ã? (Namaste, kaise ho?)  or  ‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç? (Aap kaise hain?) - The first is informal, the second is more formal.\n",
            "\n",
            "\n",
            "2. **What is your name? (Comment tu t'appelles ?)** ‚Üí ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à? (Tumhara naam kya hai?) - Informal. ‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à? (Aapka naam kya hai?) - Formal.\n",
            "\n",
            "\n",
            "3. **I love programming.** ‚Üí ‡§Æ‡•Å‡§ù‡•á ‡§™‡•ç‡§∞‡•ã‡§ó‡•ç‡§∞‡§æ‡§Æ‡§ø‡§Ç‡§ó ‡§¨‡§π‡•Å‡§§ ‡§™‡§∏‡§Ç‡§¶ ‡§π‡•à‡•§ (Mujhe programming bahut pasand hai.)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Instruction-style Prompting"
      ],
      "metadata": {
        "id": "2MwoLJptpZF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üî∑ Instruction-style: Direct command\n",
        "prompt_instruction = \"Summarize the following: 'The Great Wall of China was built to protect Chinese states from invasions. It spans over 13,000 miles.'\"\n",
        "\n",
        "response_instruction = model.generate_content(prompt_instruction)\n",
        "print(\"\\nüî∑ Instruction-style Output:\\n\", response_instruction.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "-aV88l-bos8y",
        "outputId": "73a574a5-5159-4e55-e917-0637685054c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üî∑ Instruction-style Output:\n",
            " The Great Wall of China, a structure over 13,000 miles long, was built to defend Chinese states against invaders.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Chain-of-Thought Prompting"
      ],
      "metadata": {
        "id": "tUOkOHE1pbbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üî∂ Chain-of-Thought: Ask to reason step-by-step\n",
        "prompt_cot = \"Q: A train travels at 60 km/h. How far will it go in 3 hours?\\nA: Let's think step-by-step.\"\n",
        "\n",
        "response_cot = model.generate_content(prompt_cot)\n",
        "print(\"\\nüî∂ Chain-of-Thought Output:\\n\", response_cot.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "TE02egluo4io",
        "outputId": "8cfa438c-3e8e-4bbe-eb2b-14cb6cc04af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üî∂ Chain-of-Thought Output:\n",
            " The train's speed is 60 km/h, and it travels for 3 hours.  To find the distance, we multiply speed by time:\n",
            "\n",
            "Distance = Speed √ó Time\n",
            "Distance = 60 km/h √ó 3 h\n",
            "Distance = 180 km\n",
            "\n",
            "The train will go 180 km in 3 hours.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Role-based Prompting"
      ],
      "metadata": {
        "id": "y6oZWy1jpd3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üü£ Role-based: Ask model to take a specific persona\n",
        "prompt_role = \"You are an experienced data scientist. Explain overfitting in simple terms.\"\n",
        "\n",
        "response_role = model.generate_content(prompt_role)\n",
        "print(\"\\nüü£ Role-based Output:\\n\", response_role.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "klvfjc_Bo-Uc",
        "outputId": "15325631-6af9-4f47-e663-4178411a3447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üü£ Role-based Output:\n",
            " Imagine you're teaching a parrot to talk.  You show it a picture of a cat and say \"cat\".  You show it another picture of a cat, and say \"cat\".  You do this dozens of times, with slightly different cat pictures. The parrot learns to say \"cat\" really well whenever it sees a cat picture *like the ones you showed it*.\n",
            "\n",
            "Now, you show it a picture of a cat it's *never* seen before ‚Äì maybe it's a different breed, a different angle, or a different setting.  The parrot might not recognize it as a cat!  It's memorized the *specific details* of the pictures you showed it, instead of learning the general concept of \"cat\".  That's overfitting.\n",
            "\n",
            "In data science, overfitting happens when a model learns the training data *too well*.  It memorizes the noise and quirks in the training data, rather than learning the underlying patterns that generalize to new, unseen data.  This means it performs great on the training data but poorly on new data ‚Äì just like our parrot.\n",
            "\n",
            "The solution?  Simplify the model (make the parrot focus on bigger, more general features), use more data (show the parrot more diverse cat pictures), or use techniques like regularization (make the parrot pay less attention to small details).  The goal is to find the right balance: a model that learns enough to be accurate, but not so much that it memorizes the training data and fails to generalize.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}